# Counterfactual Narrative Explainer Web Application

A modern, full-stack web application for generating human-readable counterfactual narrative explanations using Large Language Models (LLMs). This application provides an intuitive interface for exploring how changes in input features affect model predictions through natural language explanations.

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **Node.js 16+**
- **Git** (for submodule management)
- **GPU with CUDA** (optional, for vLLM models)
- **Google API Key** (optional, for Gemini models)

### Initial Setup

1. **Clone the repository with submodules:**
   ```bash
   git clone --recurse-submodules <repository-url>
   cd xai-narrative-webapp
   ```

   If you already cloned without submodules:
   ```bash
   git submodule update --init --recursive
   ```

2. **Start the backend:**
   ```bash
   cd webapp
   ./start-backend.sh
   ```

3. **Start the frontend** (in a new terminal):
   ```bash
   cd webapp
   ./start-frontend.sh
   ```

4. **Open your browser:**
   Navigate to `http://localhost:5173`

## ğŸ“ Project Structure

```
xai-narrative-webapp/
â”œâ”€â”€ llm_kd/                    # Git submodule - LLM pipeline repository
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ utils.py           # MODEL_MAPPING and prompt templates
â”‚   â”‚   â””â”€â”€ pipeline.py        # LLM pipeline implementation
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ dataset_kb.py      # Dataset knowledge base
â”œâ”€â”€ webapp/                    # Main application directory
â”‚   â”œâ”€â”€ backend/               # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ api/              # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ frontend/             # React frontend
â”‚       â”œâ”€â”€ src/              # React components
â”‚       â””â”€â”€ package.json     # Node dependencies
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ SETUP.md                  # Detailed setup guide
â””â”€â”€ .gitmodules              # Git submodule configuration
```

## âœ¨ Features

- **ğŸ¯ Dataset Support**: Adult Income, Titanic, California Housing, Diabetes
- **ğŸ¤– Multi-Model Support**: 
  - vLLM models (local GPU inference)
  - Google Gemini models (cloud API)
- **ğŸ”„ Dynamic Counterfactuals**: Load new counterfactuals for the same factual instance
- **ğŸ“Š Visual Comparison**: Side-by-side comparison with highlighted differences
- **ğŸ“ Narrative Explanations**: Human-readable explanations generated by LLMs
- **ğŸ’¾ History Tracking**: View recent explanations (stored locally)
- **ğŸ“‹ Copy to Clipboard**: Easy sharing of explanations
- **ğŸŒ™ Dark Theme**: Modern, professional dark UI

## ğŸ› ï¸ Technology Stack

### Backend
- **FastAPI**: Modern Python web framework
- **vLLM**: High-performance LLM inference (optional)
- **Google Generative AI**: Gemini API integration (optional)
- **Pydantic**: Data validation

### Frontend
- **React 18**: UI framework
- **Vite**: Build tool and dev server
- **Tailwind CSS**: Utility-first CSS framework
- **Axios**: HTTP client

## ğŸ“š Documentation

- **[Webapp README](webapp/README.md)**: Detailed documentation for the webapp
- **[Quick Start Guide](webapp/QUICKSTART.md)**: Step-by-step setup instructions
- **[Setup Guide](SETUP.md)**: llm_kd integration and submodule management

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in `webapp/backend/`:

```bash
# GPU Configuration (for vLLM)
TENSOR_PARALLEL_SIZE=1
GPU_MEMORY_UTILIZATION=0.9

# Gemini Configuration (for Google Generative AI)
GOOGLE_API_KEY=your_api_key_here
GEMINI_MODEL_NAME=gemini-2.0-flash-exp

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
```

## ğŸ”„ Updating the llm_kd Submodule

To sync with the latest changes from the llm_kd repository:

```bash
cd llm_kd
git pull origin main  # or your branch
cd ..
git add llm_kd
git commit -m "Update llm_kd submodule"
```

## ğŸ§ª Usage

1. **Select a Dataset**: Choose from Adult Income, Titanic, California Housing, or Diabetes
2. **Select a Model**: 
   - Choose a vLLM model (requires GPU)
   - Or choose a Gemini model (requires API key)
3. **Load Example**: Click "Load Example" to fetch a factual/counterfactual pair
4. **New Counterfactual**: Click "New Counterfactual" to get a different counterfactual for the same factual
5. **Generate Explanation**: Click "Generate Explanation" to create a narrative explanation
6. **View Results**: See the explanation, feature changes, and target variable changes

## ğŸ› Troubleshooting

### Submodule Issues

- **Submodule not initialized**: Run `git submodule update --init --recursive`
- **Submodule appears empty**: Check `.gitmodules` file and verify the remote URL

### Backend Issues

- **Import errors**: Ensure llm_kd submodule is initialized
- **MODEL_MAPPING empty**: Check backend logs for import messages
- **Port 8000 in use**: Change port in `start-backend.sh` or kill the process

### Frontend Issues

- **Port 5173 in use**: Vite will automatically use next available port
- **API connection errors**: Ensure backend is running on port 8000

See [webapp/README.md](webapp/README.md) for more detailed troubleshooting.

## ğŸ“ License

This project is part of a research project. Please refer to the main repository for license information.

## ğŸ¤ Contributing

1. Ensure llm_kd submodule is up to date
2. Make changes in the appropriate directory (`webapp/backend/` or `webapp/frontend/`)
3. Test your changes locally
4. Submit a pull request

## ğŸ“– Additional Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [React Documentation](https://react.dev/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [Google Generative AI](https://ai.google.dev/)

## ğŸ¯ Project Goals

This application aims to:
- Provide an intuitive interface for exploring counterfactual explanations
- Support multiple LLM backends (local and cloud)
- Generate high-quality, human-readable narrative explanations
- Enable easy comparison of different counterfactual scenarios

---

For detailed setup instructions, see [webapp/QUICKSTART.md](webapp/QUICKSTART.md) or [SETUP.md](SETUP.md).

