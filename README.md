# Narrative Explanation Systems with SLMs

A modern, full-stack web application for generating human-readable counterfactual narrative explanations using Large Language Models (LLMs). This application provides an intuitive interface for exploring how changes in input features affect model predictions through natural language explanations.

![Pipeline](img/pipeline.jpg)

![User Interface](img/UI.jpg)

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **Node.js 16+**
- **Git** (for submodule management)
- **GPU with CUDA** (optional, for vLLM models)

### Initial Setup

1. **Clone the repository with submodules:**
   ```bash
   git clone --recurse-submodules <repository-url>
   cd xai-narrative-webapp
   ```

   If you already cloned without submodules:
   ```bash
   git submodule update --init --recursive
   ```

2. **Start the backend:**
   ```bash
   cd webapp
   ./start-backend.sh
   ```

3. **Start the frontend** (in a new terminal):
   ```bash
   cd webapp
   ./start-frontend.sh
   ```

4. **Open your browser:**
   Navigate to `http://localhost:5173`

## âœ¨ Features

- **ğŸ¯ Dataset Support**: Adult Income, Titanic, California Housing, Diabetes
- **ğŸ¤– Multi-Model Support**: 
  - vLLM models (local GPU inference)
- **ğŸ”„ Dynamic Counterfactuals**: Load new counterfactuals for the same factual instance
- **ğŸ“Š Visual Comparison**: Side-by-side comparison with highlighted differences
- **ğŸ“ Narrative Explanations**: Human-readable explanations generated by LLMs
- **ğŸŒ“ Light/Dark Theme**: Toggle between light and dark themes
- **ğŸ¨ Fine-tuned Models**: Option to run fine-tuned models (available on [Hugging Face](https://huggingface.co/phdsilver22/models) or base models
- **ğŸ”§ Refiner Pipeline**: Toggle refiner pipeline on or off
- **ğŸ“‹ Copy to Clipboard**: Easy sharing of explanations

## ğŸ“ Project Structure

```
xai-narrative-webapp/
â”œâ”€â”€ llm_kd/                    # Git submodule - LLM pipeline repository
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ utils.py           # MODEL_MAPPING and prompt templates
â”‚   â”‚   â””â”€â”€ pipeline.py        # LLM pipeline implementation
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ dataset_kb.py      # Dataset knowledge base
â”œâ”€â”€ webapp/                    # Main application directory
â”‚   â”œâ”€â”€ backend/               # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ api/              # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ frontend/             # React frontend
â”‚       â”œâ”€â”€ src/              # React components
â”‚       â””â”€â”€ package.json     # Node dependencies
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ SETUP.md                  # Detailed setup guide
â””â”€â”€ .gitmodules              # Git submodule configuration
```

## ğŸ“š Documentation

- **[Webapp README](webapp/README.md)**: Detailed documentation for the webapp
- **[Quick Start Guide](webapp/QUICKSTART.md)**: Step-by-step setup instructions
- **[Setup Guide](SETUP.md)**: llm_kd integration and submodule management

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in `webapp/backend/`:

```bash
# GPU Configuration (for vLLM)
TENSOR_PARALLEL_SIZE=1
GPU_MEMORY_UTILIZATION=0.9

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
```

## ğŸ”„ Updating the llm_kd Submodule

To sync with the latest changes from the llm_kd repository:

```bash
cd llm_kd
git pull origin main  # or your branch
cd ..
git add llm_kd
git commit -m "Update llm_kd submodule"
```

## ğŸ§ª Usage

1. **Select a Dataset**: Choose from Adult Income, Titanic, California Housing, or Diabetes
2. **Select a Model**: Choose a vLLM model (requires GPU)
3. **Load Example**: Click "Load Example" to fetch a factual/counterfactual pair
4. **New Counterfactual**: Click "New Counterfactual" to get a different counterfactual for the same factual
5. **Generate Explanation**: Click "Generate Explanation" to create a narrative explanation
6. **View Results**: See the explanation, feature changes, and target variable changes

## ğŸ¤ Contributing

1. Ensure llm_kd submodule is up to date
2. Make changes in the appropriate directory (`webapp/backend/` or `webapp/frontend/`)
3. Test your changes locally
4. Submit a pull request

## ğŸ¯ Project Goals

This application aims to:
- Provide an intuitive interface for exploring counterfactual explanations
- Support multiple LLM backends (local and cloud)
- Generate high-quality, human-readable narrative explanations
- Enable easy comparison of different counterfactual scenarios

---

For detailed setup instructions, see [webapp/QUICKSTART.md](webapp/QUICKSTART.md) or [SETUP.md](SETUP.md).

